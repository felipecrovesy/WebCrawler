<div align="center">
  <h1 align="center">WebCrawl</h1>

   <div align="center">
     Ã‰ um programa de computador que navega pela internet de forma automatizada, coletando informaÃ§Ãµes de pÃ¡ginas da web.
    </div>
</div>

## ğŸ“‹ <a name="table">Sumary</a>

1. ğŸ¤– [IntroduÃ§Ã£o](#introduction)
2. âš™ï¸ [Tech Stack](#tech-stack)
3. ğŸ”‹ [Recursos](#features)
4. ğŸ¤¸ [Como usar](#quick-start)
5. ğŸ’¾ [VariÃ¡veis de Ambiente](#envs)
6. ğŸ“… [VersÃµes](#versions)
7. ğŸ¤ [ContribuiÃ§Ãµes](#contributing)
8. ğŸ‘¥ [Autores](#authors)
<br>

## <a name="introduction">ğŸ¤– IntroduÃ§Ã£o</a>

ComeÃ§o da jornada: Um web crawler inicia sua jornada a partir de uma lista de URLs, que sÃ£o os endereÃ§os das pÃ¡ginas da web.

Seguindo os links: Ele acessa essas pÃ¡ginas e identifica todos os links presentes nelas.

ExploraÃ§Ã£o contÃ­nua: Em seguida, o crawler segue esses links, explorando novas pÃ¡ginas e repetindo o processo.

Coleta de informaÃ§Ãµes: Ao visitar cada pÃ¡gina, o web crawler extrai informaÃ§Ãµes relevantes, como texto, imagens, vÃ­deos e outros tipos de conteÃºdo.

IndexaÃ§Ã£o dos dados: As informaÃ§Ãµes coletadas sÃ£o organizadas e armazenadas em um banco de dados, formando um Ã­ndice que facilita a busca e o acesso aos dados.

<br>

**Para que servem os web crawlers**:
___
+ Mecanismos de busca: Os mecanismos de busca, como o Google, utilizam web crawlers para indexar a internet e construir seus Ã­ndices de pesquisa.
+ Monitoramento de sites: Web crawlers podem ser usados para monitorar sites em busca de alteraÃ§Ãµes, como novas publicaÃ§Ãµes ou atualizaÃ§Ãµes de conteÃºdo.
+ Coleta de dados: Empresas e pesquisadores podem utilizar web crawlers para coletar dados da web para anÃ¡lise, estudos de mercado e outras finalidades.
+ ComparaÃ§Ã£o de preÃ§os: Sites de comparaÃ§Ã£o de preÃ§os utilizam web crawlers para coletar informaÃ§Ãµes sobre produtos e preÃ§os em diferentes lojas online.


<br>

**Outros nomes para Webcrawler**:
____
+ Crawler
+ Spider
+ Bot
+ Web robot
+ Indexador automÃ¡tico

  <br>

## <a name="tech-stack">âš™ï¸ Tech Stack</a>

- dotnet 9.0
- Sqlite
  
<br>

## <a name="quick-start">ğŸ¤¸ Como usar</a>

Para iniciar o projeto, siga os seguintes passos em seu dispositivo:

**00 - PrÃ©-requisitos**

Para usar este projeto vocÃª deve ter instalado previamente os seguintes pacotes:

- [Git](https://git-scm.com/)
- [dotnet 9.0](https://dotnet.microsoft.com/pt-br/download/dotnet/9.0)
- [Chromium](https://www.chromium.org/getting-involved/download-chromium/)
- [Sqlite](https://www.sqlite.org/download.html) (opcional para visualizar a tabela)
  <br/><br/>

**01 - Clonar o RepositÃ³rio**

```bash
git clone https://github.com/felipecrovesy/WebCrawler.git
```

**02 - Rodar o Projeto**

```bash
dotnet run WebCrawler.Presentation
```

**03 - Rodar os Tests**

Acesse WebCrawler.UnitTest via terminal

```bash
dotnet test
```

## <a name="constributing">ğŸ¤ ContribuiÃ§Ãµes</a>

ContriguiÃ§Ãµes, issues, e novos recursos sÃ£o vem vindos!

1. FaÃ§a um Fork do projeto (<https://github.com/yourname/yourproject/fork>)
2. Crie a sua branch de feature (`git checkout -b feature/fooBar`)
3. Commit suas alteraÃ§Ãµes (`git commit -am 'Add some fooBar'`)
4. FaÃ§a um Push para a branch (`git push origin feature/fooBar`)
5. Crie um novo Pull Request


## <a name="authors">ğŸ‘¥ Autores</a>

<table style="border-collapse: collapse; table-layout: auto; text-align: left;">

  <tbody>
    <tr>
      <td style="padding: 10px; border: 1px solid #ddd;">
        <img src="https://avatars.githubusercontent.com/u/60819196?v=4" width="60" style="border-radius: 50%; display: block; margin: 0 auto;">
      </td>
      <td style="padding: 10px; border: 1px solid #ddd;">Felipe Crovesy</td>
      <td style="padding: 10px; border: 1px solid #ddd;">
        <a href="https://www.linkedin.com/in/felipe-crovesy-6a299283/" target="_blank">LinkedIn</a> |
        <a href="https://github.com/felipecrovesy" target="_blank">GitHub</a>
      </td>
    </tr>
  </tbody>
</table>
